{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8UzSEMoIFS_D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d752722-b12e-49d6-8d4e-3f236619f056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Requirement already satisfied: shimmy<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (0.2.1)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (0.8.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (6.4.4)\n",
            "Requirement already satisfied: gymnasium[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (0.0.4)\n",
            "Requirement already satisfied: autorom~=0.4.2 in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (4.66.5)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2024.7.4)\n",
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ale-py) (1.26.4)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py) (6.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py) (4.12.2)\n",
            "Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.10/dist-packages (2.3.2)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.4.0+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2024.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: imageio[ffmpeg] in /usr/local/lib/python3.10/dist-packages (2.34.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio[ffmpeg]) (1.26.4)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio[ffmpeg]) (9.4.0)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (from imageio[ffmpeg]) (0.5.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from imageio[ffmpeg]) (5.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg->imageio[ffmpeg]) (71.0.4)\n"
          ]
        }
      ],
      "source": [
        "# Requirements\n",
        "!pip install gymnasium[atari]\n",
        "!pip install gymnasium[accept-rom-license]\n",
        "!pip install ale-py\n",
        "!pip install stable-baselines3\n",
        "!pip install imageio[ffmpeg]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
        "import ale_py\n",
        "from gymnasium.wrappers import TransformReward, TimeLimit\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "C8N6xcEFFZMd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing the custom reward that includes an empty space reward\n",
        "#class CustomRewardWrapper(gym.Wrapper):\n",
        "    #def __init__(self, env):\n",
        "        #super().__init__(env)\n",
        "        # Counters for specific actions taken by the agent\n",
        "        #self.fire_count = 0\n",
        "        #self.left_right_count = 0\n",
        "\n",
        "        # Thresholds for applying penalties and rewards\n",
        "        #self.left_right_threshold = 100\n",
        "        #self.fire_penalty_threshold = 500\n",
        "\n",
        "        # Penalties and rewards definitions\n",
        "       # self.fire_penalty = -0.01  # Penalty for excessive \"FIRE\" actions\n",
        "        #self.termination_penalty = -0.1  # Penalty for game over\n",
        "        #self.movement_reward = 0.01  # Reward for moving left or right\n",
        "       # self.survival_bonus = 0.005  # Bonus for surviving longer\n",
        "\n",
        "    #def reset(self, **kwargs):\n",
        "        # Reset counters when a new episode starts\n",
        "       # self.fire_count = 0\n",
        "        #self.left_right_count = 0\n",
        "        #return self.env.reset(**kwargs)\n",
        "\n",
        "    #def step(self, action):\n",
        "        # Execute the action and observe the outcome\n",
        "        #obs_before, _, _, _, _ = self.env.step(0)  # Taking a NOOP action to get the state without changing it\n",
        "        #obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "        # Evaluate custom rewards and penalties\n",
        "        #reward += self.reward_for_empty_space(obs_before, obs)\n",
        "        #reward += self.manage_action_rewards(action, obs, terminated)\n",
        "\n",
        "        #return obs, reward, terminated, truncated, info\n",
        "\n",
        "   # def reward_for_empty_space(self, obs_before, obs_after):\n",
        "       # # Evaluate the placement of a block in a completely empty column\n",
        "        #gray_before = np.mean(obs_before, axis=-1) > 0\n",
        "        #gray_after = np.mean(obs_after, axis=-1) > 0\n",
        "       # new_blocks = np.bitwise_and(np.bitwise_not(gray_before), gray_after)\n",
        "\n",
        "        #reward = 0\n",
        "      #  for col in range(new_blocks.shape[1]):\n",
        "           # if np.all(np.bitwise_not(gray_before[:, col])) and np.any(new_blocks[:, col]):\n",
        "               # reward += 0.1  # Reward for placing in an empty column\n",
        "        #return reward\n",
        "\n",
        "    #def manage_action_rewards(self, action, obs, terminated):\n",
        "        # Handle rewards and penalties based on action types\n",
        "        #reward = 0\n",
        "        #if action == 1:\n",
        "         #   self.fire_count += 1\n",
        "           # if self.fire_count > self.fire_penalty_threshold:\n",
        "               # reward += self.fire_penalty\n",
        "\n",
        "        #if action == 2 or action == 3:\n",
        "           # self.left_right_count += 1\n",
        "            #if self.left_right_count <= self.left_right_threshold:\n",
        "                #reward += self.movement_reward\n",
        "\n",
        "        #if terminated:\n",
        "            #reward += self.termination_penalty\n",
        "\n",
        "       # reward += self.survival_bonus  # Incremental reward for each step\n",
        "        #return reward"
      ],
      "metadata": {
        "id": "JkIYI-dHLjn1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create custom reward and penalty functions\n",
        "class CustomRewardWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        # Counters for specific actions taken by the agent\n",
        "        self.fire_count = 0\n",
        "        self.left_right_count = 0\n",
        "\n",
        "        # Thresholds for applying penalties and rewards\n",
        "        self.left_right_threshold = 100\n",
        "        self.fire_penalty_threshold = 500\n",
        "\n",
        "        # Penalties and rewards definitions\n",
        "        self.fire_penalty = -0.01  # Penalty for excessive \"FIRE\" actions\n",
        "        self.termination_penalty = -0.1  # Penalty for game over\n",
        "        self.movement_reward = 0.01  # Reward for moving left or right\n",
        "\n",
        "        # Additional bonuses\n",
        "        self.survival_bonus = 0.005  # Bonus for surviving longer\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        # Reset counters when a new episode starts\n",
        "        self.fire_count = 0\n",
        "        self.left_right_count = 0\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "    def step(self, action):\n",
        "        # Execute the action and observe the outcome\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "        # Increment fire count and apply penalty if threshold is exceeded\n",
        "        if action == 1:\n",
        "            self.fire_count += 1\n",
        "            if self.fire_count > self.fire_penalty_threshold:\n",
        "                reward += self.fire_penalty  # Penalize for excessive fire actions\n",
        "\n",
        "        # Increment left/right count and reward movement within threshold\n",
        "        if action == 2 or action == 3:\n",
        "            self.left_right_count += 1\n",
        "            if self.left_right_count <= self.left_right_threshold:\n",
        "                reward += self.movement_reward  # Reward for moving left or right\n",
        "\n",
        "        # Apply bonuses for gameplay strategies\n",
        "        reward += self.survival_bonus  # Bonus for each step the game continues\n",
        "\n",
        "        # Apply termination penalty if the game ends\n",
        "        if terminated:\n",
        "            reward += self.termination_penalty\n",
        "\n",
        "        return obs, reward, terminated, truncated, info"
      ],
      "metadata": {
        "id": "1GuPMEOBFbS9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the Atari environment\n",
        "gym.register_envs(ale_py)\n",
        "\n",
        "# Create the Tetris environment and modify with time limit and custom class\n",
        "env = gym.make(\"ALE/Tetris-v5\")\n",
        "#env = TimeLimit(env, max_episode_steps=5000)  # Limit episodes to 5000 steps\n",
        "env = CustomRewardWrapper(env)\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to the model\n",
        "model_path = \"/content/drive/My Drive/tetris_checkpoints/best_model_cnn_exp_v1.zip\"\n",
        "save_path = \"/content/drive/My Drive/tetris_checkpoints/\"\n",
        "\n",
        "# Check if the model exists and load or create a new one\n",
        "if os.path.exists(model_path):\n",
        "    print(\"Loading existing model to continue training.\")\n",
        "    model = DQN.load(model_path, env=env)\n",
        "else:\n",
        "    print(\"No existing model found, training a new model.\")\n",
        "    model = DQN(\n",
        "        policy=\"CnnPolicy\",\n",
        "        env=env,\n",
        "        learning_rate=1e-4,\n",
        "        buffer_size=200000,\n",
        "        learning_starts=20000,\n",
        "        batch_size=32,\n",
        "        tau=1.0,\n",
        "        gamma=0.99,\n",
        "        train_freq=4,\n",
        "        target_update_interval=10000,\n",
        "        #exploration_fraction=0.1,\n",
        "        #exploration_final_eps=0.02,\n",
        "        exploration_initial_eps = 1.0,  # Start with full exploration (for testing)\n",
        "        exploration_final_eps = 0.1,    # End with 10% random actions(for testing)\n",
        "        exploration_fraction = 0.5,     # Extend the exploration period to 50% of total training (for testing)\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "# Callback to save the best model\n",
        "eval_callback = EvalCallback(\n",
        "    env,\n",
        "    best_model_save_path=save_path,\n",
        "    log_path=save_path,\n",
        "    eval_freq=10000,\n",
        "    deterministic=True,\n",
        "    render=False\n",
        ")\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs('/content/tetris_checkpoints/', exist_ok=True)\n",
        "\n",
        "# Train the model\n",
        "model.learn(total_timesteps=200000, callback=[eval_callback])\n",
        "\n",
        "# Save the final model\n",
        "final_model_path = os.path.join(save_path, \"tetris_cnn_exp_v1\")\n",
        "model.save(final_model_path)\n",
        "print(\"Training complete and final model saved at\", final_model_path)\n",
        "\n",
        "# Evaluate the trained model\n",
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
        "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")\n",
        "\n",
        "# Close the environment\n",
        "env.close()"
      ],
      "metadata": {
        "id": "1Y2O1TOcFhmn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f768fad-8e20-4a15-9846-b595fb449efc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "No existing model found, training a new model.\n",
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Wrapping the env in a VecTransposeImage.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py:414: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x7e10144550c0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7e1005fdb250>\n",
            "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 582      |\n",
            "|    ep_rew_mean      | 4.06     |\n",
            "|    exploration_rate | 0.979    |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 766      |\n",
            "|    time_elapsed     | 3        |\n",
            "|    total_timesteps  | 2330     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 582      |\n",
            "|    ep_rew_mean      | 3.93     |\n",
            "|    exploration_rate | 0.958    |\n",
            "| time/               |          |\n",
            "|    episodes         | 8        |\n",
            "|    fps              | 787      |\n",
            "|    time_elapsed     | 5        |\n",
            "|    total_timesteps  | 4653     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 554      |\n",
            "|    ep_rew_mean      | 3.75     |\n",
            "|    exploration_rate | 0.94     |\n",
            "| time/               |          |\n",
            "|    episodes         | 12       |\n",
            "|    fps              | 794      |\n",
            "|    time_elapsed     | 8        |\n",
            "|    total_timesteps  | 6650     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 547      |\n",
            "|    ep_rew_mean      | 3.7      |\n",
            "|    exploration_rate | 0.921    |\n",
            "| time/               |          |\n",
            "|    episodes         | 16       |\n",
            "|    fps              | 799      |\n",
            "|    time_elapsed     | 10       |\n",
            "|    total_timesteps  | 8746     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=3.80 +/- 0.07\n",
            "Episode length: 580.20 +/- 14.40\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 580      |\n",
            "|    mean_reward      | 3.8      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.91     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 10000    |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 577      |\n",
            "|    ep_rew_mean      | 3.89     |\n",
            "|    exploration_rate | 0.896    |\n",
            "| time/               |          |\n",
            "|    episodes         | 20       |\n",
            "|    fps              | 551      |\n",
            "|    time_elapsed     | 20       |\n",
            "|    total_timesteps  | 11548    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 571      |\n",
            "|    ep_rew_mean      | 3.84     |\n",
            "|    exploration_rate | 0.877    |\n",
            "| time/               |          |\n",
            "|    episodes         | 24       |\n",
            "|    fps              | 579      |\n",
            "|    time_elapsed     | 23       |\n",
            "|    total_timesteps  | 13715    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 566      |\n",
            "|    ep_rew_mean      | 3.8      |\n",
            "|    exploration_rate | 0.857    |\n",
            "| time/               |          |\n",
            "|    episodes         | 28       |\n",
            "|    fps              | 602      |\n",
            "|    time_elapsed     | 26       |\n",
            "|    total_timesteps  | 15849    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 554      |\n",
            "|    ep_rew_mean      | 3.73     |\n",
            "|    exploration_rate | 0.84     |\n",
            "| time/               |          |\n",
            "|    episodes         | 32       |\n",
            "|    fps              | 617      |\n",
            "|    time_elapsed     | 28       |\n",
            "|    total_timesteps  | 17723    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 547      |\n",
            "|    ep_rew_mean      | 3.69     |\n",
            "|    exploration_rate | 0.823    |\n",
            "| time/               |          |\n",
            "|    episodes         | 36       |\n",
            "|    fps              | 633      |\n",
            "|    time_elapsed     | 31       |\n",
            "|    total_timesteps  | 19682    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=3.80 +/- 0.07\n",
            "Episode length: 580.20 +/- 14.40\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 580      |\n",
            "|    mean_reward      | 3.8      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.82     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 20000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 560      |\n",
            "|    ep_rew_mean      | 3.77     |\n",
            "|    exploration_rate | 0.798    |\n",
            "| time/               |          |\n",
            "|    episodes         | 40       |\n",
            "|    fps              | 483      |\n",
            "|    time_elapsed     | 46       |\n",
            "|    total_timesteps  | 22403    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.15e-06 |\n",
            "|    n_updates        | 600      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 562      |\n",
            "|    ep_rew_mean      | 3.78     |\n",
            "|    exploration_rate | 0.777    |\n",
            "| time/               |          |\n",
            "|    episodes         | 44       |\n",
            "|    fps              | 450      |\n",
            "|    time_elapsed     | 54       |\n",
            "|    total_timesteps  | 24749    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.5e-06  |\n",
            "|    n_updates        | 1187     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 563      |\n",
            "|    ep_rew_mean      | 3.78     |\n",
            "|    exploration_rate | 0.757    |\n",
            "| time/               |          |\n",
            "|    episodes         | 48       |\n",
            "|    fps              | 426      |\n",
            "|    time_elapsed     | 63       |\n",
            "|    total_timesteps  | 27018    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.72e-06 |\n",
            "|    n_updates        | 1754     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 564      |\n",
            "|    ep_rew_mean      | 3.78     |\n",
            "|    exploration_rate | 0.736    |\n",
            "| time/               |          |\n",
            "|    episodes         | 52       |\n",
            "|    fps              | 407      |\n",
            "|    time_elapsed     | 72       |\n",
            "|    total_timesteps  | 29353    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.33e-06 |\n",
            "|    n_updates        | 2338     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=3.26 +/- 0.00\n",
            "Episode length: 473.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 473      |\n",
            "|    mean_reward      | 3.26     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.73     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 30000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.15e-06 |\n",
            "|    n_updates        | 2499     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 571      |\n",
            "|    ep_rew_mean      | 3.81     |\n",
            "|    exploration_rate | 0.712    |\n",
            "| time/               |          |\n",
            "|    episodes         | 56       |\n",
            "|    fps              | 368      |\n",
            "|    time_elapsed     | 86       |\n",
            "|    total_timesteps  | 31954    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.96e-06 |\n",
            "|    n_updates        | 2988     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 574      |\n",
            "|    ep_rew_mean      | 3.82     |\n",
            "|    exploration_rate | 0.69     |\n",
            "| time/               |          |\n",
            "|    episodes         | 60       |\n",
            "|    fps              | 357      |\n",
            "|    time_elapsed     | 96       |\n",
            "|    total_timesteps  | 34420    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.86e-06 |\n",
            "|    n_updates        | 3604     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 572      |\n",
            "|    ep_rew_mean      | 3.81     |\n",
            "|    exploration_rate | 0.67     |\n",
            "| time/               |          |\n",
            "|    episodes         | 64       |\n",
            "|    fps              | 349      |\n",
            "|    time_elapsed     | 104      |\n",
            "|    total_timesteps  | 36612    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.15e-06 |\n",
            "|    n_updates        | 4152     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 573      |\n",
            "|    ep_rew_mean      | 3.81     |\n",
            "|    exploration_rate | 0.649    |\n",
            "| time/               |          |\n",
            "|    episodes         | 68       |\n",
            "|    fps              | 342      |\n",
            "|    time_elapsed     | 113      |\n",
            "|    total_timesteps  | 38978    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3e-06    |\n",
            "|    n_updates        | 4744     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=5.60 +/- 0.00\n",
            "Episode length: 941.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 941      |\n",
            "|    mean_reward      | 5.6      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.64     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 40000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.56e-06 |\n",
            "|    n_updates        | 4999     |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 579      |\n",
            "|    ep_rew_mean      | 3.87     |\n",
            "|    exploration_rate | 0.625    |\n",
            "| time/               |          |\n",
            "|    episodes         | 72       |\n",
            "|    fps              | 308      |\n",
            "|    time_elapsed     | 135      |\n",
            "|    total_timesteps  | 41702    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.63e-06 |\n",
            "|    n_updates        | 5425     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 585      |\n",
            "|    ep_rew_mean      | 3.89     |\n",
            "|    exploration_rate | 0.6      |\n",
            "| time/               |          |\n",
            "|    episodes         | 76       |\n",
            "|    fps              | 303      |\n",
            "|    time_elapsed     | 146      |\n",
            "|    total_timesteps  | 44431    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.17e-06 |\n",
            "|    n_updates        | 6107     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 588      |\n",
            "|    ep_rew_mean      | 3.91     |\n",
            "|    exploration_rate | 0.577    |\n",
            "| time/               |          |\n",
            "|    episodes         | 80       |\n",
            "|    fps              | 300      |\n",
            "|    time_elapsed     | 156      |\n",
            "|    total_timesteps  | 47047    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.76e-06 |\n",
            "|    n_updates        | 6761     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 589      |\n",
            "|    ep_rew_mean      | 3.91     |\n",
            "|    exploration_rate | 0.555    |\n",
            "| time/               |          |\n",
            "|    episodes         | 84       |\n",
            "|    fps              | 297      |\n",
            "|    time_elapsed     | 166      |\n",
            "|    total_timesteps  | 49491    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.58e-06 |\n",
            "|    n_updates        | 7372     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=6.10 +/- 0.16\n",
            "Episode length: 1041.00 +/- 32.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1.04e+03 |\n",
            "|    mean_reward      | 6.1      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.55     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 50000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 9.23e-06 |\n",
            "|    n_updates        | 7499     |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 593      |\n",
            "|    ep_rew_mean      | 3.94     |\n",
            "|    exploration_rate | 0.53     |\n",
            "| time/               |          |\n",
            "|    episodes         | 88       |\n",
            "|    fps              | 275      |\n",
            "|    time_elapsed     | 189      |\n",
            "|    total_timesteps  | 52188    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.06e-05 |\n",
            "|    n_updates        | 8046     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 595      |\n",
            "|    ep_rew_mean      | 3.94     |\n",
            "|    exploration_rate | 0.507    |\n",
            "| time/               |          |\n",
            "|    episodes         | 92       |\n",
            "|    fps              | 274      |\n",
            "|    time_elapsed     | 199      |\n",
            "|    total_timesteps  | 54764    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.26e-06 |\n",
            "|    n_updates        | 8690     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 599      |\n",
            "|    ep_rew_mean      | 3.96     |\n",
            "|    exploration_rate | 0.483    |\n",
            "| time/               |          |\n",
            "|    episodes         | 96       |\n",
            "|    fps              | 272      |\n",
            "|    time_elapsed     | 210      |\n",
            "|    total_timesteps  | 57480    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.25e-06 |\n",
            "|    n_updates        | 9369     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 597      |\n",
            "|    ep_rew_mean      | 3.95     |\n",
            "|    exploration_rate | 0.463    |\n",
            "| time/               |          |\n",
            "|    episodes         | 100      |\n",
            "|    fps              | 271      |\n",
            "|    time_elapsed     | 219      |\n",
            "|    total_timesteps  | 59720    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.48e-06 |\n",
            "|    n_updates        | 9929     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=4.22 +/- 0.00\n",
            "Episode length: 665.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 665      |\n",
            "|    mean_reward      | 4.22     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.46     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 60000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.61e-06 |\n",
            "|    n_updates        | 9999     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 602      |\n",
            "|    ep_rew_mean      | 3.97     |\n",
            "|    exploration_rate | 0.437    |\n",
            "| time/               |          |\n",
            "|    episodes         | 104      |\n",
            "|    fps              | 262      |\n",
            "|    time_elapsed     | 238      |\n",
            "|    total_timesteps  | 62567    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.83e-06 |\n",
            "|    n_updates        | 10641    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 604      |\n",
            "|    ep_rew_mean      | 3.98     |\n",
            "|    exploration_rate | 0.415    |\n",
            "| time/               |          |\n",
            "|    episodes         | 108      |\n",
            "|    fps              | 261      |\n",
            "|    time_elapsed     | 248      |\n",
            "|    total_timesteps  | 65023    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.74e-06 |\n",
            "|    n_updates        | 11255    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 607      |\n",
            "|    ep_rew_mean      | 4        |\n",
            "|    exploration_rate | 0.394    |\n",
            "| time/               |          |\n",
            "|    episodes         | 112      |\n",
            "|    fps              | 260      |\n",
            "|    time_elapsed     | 257      |\n",
            "|    total_timesteps  | 67307    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.23e-06 |\n",
            "|    n_updates        | 11826    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=3.24 +/- 0.00\n",
            "Episode length: 469.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 469      |\n",
            "|    mean_reward      | 3.24     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.37     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 70000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 9.27e-06 |\n",
            "|    n_updates        | 12499    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 619      |\n",
            "|    ep_rew_mean      | 4.07     |\n",
            "|    exploration_rate | 0.364    |\n",
            "| time/               |          |\n",
            "|    episodes         | 116      |\n",
            "|    fps              | 255      |\n",
            "|    time_elapsed     | 276      |\n",
            "|    total_timesteps  | 70646    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.16e-06 |\n",
            "|    n_updates        | 12661    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 615      |\n",
            "|    ep_rew_mean      | 4.04     |\n",
            "|    exploration_rate | 0.343    |\n",
            "| time/               |          |\n",
            "|    episodes         | 120      |\n",
            "|    fps              | 254      |\n",
            "|    time_elapsed     | 286      |\n",
            "|    total_timesteps  | 73026    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 7.65e-06 |\n",
            "|    n_updates        | 13256    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 624      |\n",
            "|    ep_rew_mean      | 4.1      |\n",
            "|    exploration_rate | 0.315    |\n",
            "| time/               |          |\n",
            "|    episodes         | 124      |\n",
            "|    fps              | 254      |\n",
            "|    time_elapsed     | 299      |\n",
            "|    total_timesteps  | 76143    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.05e-06 |\n",
            "|    n_updates        | 14035    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 630      |\n",
            "|    ep_rew_mean      | 4.13     |\n",
            "|    exploration_rate | 0.29     |\n",
            "| time/               |          |\n",
            "|    episodes         | 128      |\n",
            "|    fps              | 253      |\n",
            "|    time_elapsed     | 310      |\n",
            "|    total_timesteps  | 78895    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.67e-06 |\n",
            "|    n_updates        | 14723    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=4.84 +/- 0.64\n",
            "Episode length: 789.00 +/- 128.60\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 789      |\n",
            "|    mean_reward      | 4.84     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.28     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 80000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.52e-05 |\n",
            "|    n_updates        | 14999    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 640      |\n",
            "|    ep_rew_mean      | 4.19     |\n",
            "|    exploration_rate | 0.264    |\n",
            "| time/               |          |\n",
            "|    episodes         | 132      |\n",
            "|    fps              | 246      |\n",
            "|    time_elapsed     | 331      |\n",
            "|    total_timesteps  | 81745    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.89e-06 |\n",
            "|    n_updates        | 15436    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 651      |\n",
            "|    ep_rew_mean      | 4.25     |\n",
            "|    exploration_rate | 0.237    |\n",
            "| time/               |          |\n",
            "|    episodes         | 136      |\n",
            "|    fps              | 246      |\n",
            "|    time_elapsed     | 344      |\n",
            "|    total_timesteps  | 84793    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.42e-06 |\n",
            "|    n_updates        | 16198    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 652      |\n",
            "|    ep_rew_mean      | 4.24     |\n",
            "|    exploration_rate | 0.212    |\n",
            "| time/               |          |\n",
            "|    episodes         | 140      |\n",
            "|    fps              | 245      |\n",
            "|    time_elapsed     | 356      |\n",
            "|    total_timesteps  | 87600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.9e-06  |\n",
            "|    n_updates        | 16899    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=4.68 +/- 0.00\n",
            "Episode length: 757.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 757      |\n",
            "|    mean_reward      | 4.68     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.19     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 90000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.06e-06 |\n",
            "|    n_updates        | 17499    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 658      |\n",
            "|    ep_rew_mean      | 4.28     |\n",
            "|    exploration_rate | 0.185    |\n",
            "| time/               |          |\n",
            "|    episodes         | 144      |\n",
            "|    fps              | 240      |\n",
            "|    time_elapsed     | 376      |\n",
            "|    total_timesteps  | 90532    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 7.54e-06 |\n",
            "|    n_updates        | 17632    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 665      |\n",
            "|    ep_rew_mean      | 4.32     |\n",
            "|    exploration_rate | 0.158    |\n",
            "| time/               |          |\n",
            "|    episodes         | 148      |\n",
            "|    fps              | 239      |\n",
            "|    time_elapsed     | 390      |\n",
            "|    total_timesteps  | 93521    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.39e-06 |\n",
            "|    n_updates        | 18380    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 674      |\n",
            "|    ep_rew_mean      | 4.36     |\n",
            "|    exploration_rate | 0.13     |\n",
            "| time/               |          |\n",
            "|    episodes         | 152      |\n",
            "|    fps              | 239      |\n",
            "|    time_elapsed     | 404      |\n",
            "|    total_timesteps  | 96721    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.92e-06 |\n",
            "|    n_updates        | 19180    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=3.61 +/- 0.10\n",
            "Episode length: 542.60 +/- 20.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 543      |\n",
            "|    mean_reward      | 3.61     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 100000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.63e-06 |\n",
            "|    n_updates        | 19999    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 686      |\n",
            "|    ep_rew_mean      | 4.43     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 156      |\n",
            "|    fps              | 235      |\n",
            "|    time_elapsed     | 427      |\n",
            "|    total_timesteps  | 100565   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.54e-06 |\n",
            "|    n_updates        | 20141    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 689      |\n",
            "|    ep_rew_mean      | 4.45     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 160      |\n",
            "|    fps              | 235      |\n",
            "|    time_elapsed     | 439      |\n",
            "|    total_timesteps  | 103357   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.82e-06 |\n",
            "|    n_updates        | 20839    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 698      |\n",
            "|    ep_rew_mean      | 4.49     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 164      |\n",
            "|    fps              | 234      |\n",
            "|    time_elapsed     | 453      |\n",
            "|    total_timesteps  | 106441   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000374 |\n",
            "|    n_updates        | 21610    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 702      |\n",
            "|    ep_rew_mean      | 4.51     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 168      |\n",
            "|    fps              | 234      |\n",
            "|    time_elapsed     | 465      |\n",
            "|    total_timesteps  | 109161   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.37e-06 |\n",
            "|    n_updates        | 22290    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=110000, episode_reward=5.03 +/- 0.12\n",
            "Episode length: 826.40 +/- 24.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 826      |\n",
            "|    mean_reward      | 5.03     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 110000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.09e-06 |\n",
            "|    n_updates        | 22499    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 701      |\n",
            "|    ep_rew_mean      | 4.49     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 172      |\n",
            "|    fps              | 230      |\n",
            "|    time_elapsed     | 485      |\n",
            "|    total_timesteps  | 111763   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 9.4e-06  |\n",
            "|    n_updates        | 22940    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 702      |\n",
            "|    ep_rew_mean      | 4.5      |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 176      |\n",
            "|    fps              | 230      |\n",
            "|    time_elapsed     | 497      |\n",
            "|    total_timesteps  | 114603   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.43e-06 |\n",
            "|    n_updates        | 23650    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 702      |\n",
            "|    ep_rew_mean      | 4.5      |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 180      |\n",
            "|    fps              | 230      |\n",
            "|    time_elapsed     | 509      |\n",
            "|    total_timesteps  | 117231   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.78e-06 |\n",
            "|    n_updates        | 24307    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=120000, episode_reward=4.02 +/- 0.21\n",
            "Episode length: 624.20 +/- 41.60\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 624      |\n",
            "|    mean_reward      | 4.02     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 120000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.65e-06 |\n",
            "|    n_updates        | 24999    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 712      |\n",
            "|    ep_rew_mean      | 4.56     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 184      |\n",
            "|    fps              | 227      |\n",
            "|    time_elapsed     | 531      |\n",
            "|    total_timesteps  | 120713   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.17e-05 |\n",
            "|    n_updates        | 25178    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 716      |\n",
            "|    ep_rew_mean      | 4.58     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 188      |\n",
            "|    fps              | 227      |\n",
            "|    time_elapsed     | 545      |\n",
            "|    total_timesteps  | 123783   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 9.09e-06 |\n",
            "|    n_updates        | 25945    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 715      |\n",
            "|    ep_rew_mean      | 4.57     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 192      |\n",
            "|    fps              | 227      |\n",
            "|    time_elapsed     | 555      |\n",
            "|    total_timesteps  | 126239   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.77e-06 |\n",
            "|    n_updates        | 26559    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 713      |\n",
            "|    ep_rew_mean      | 4.56     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 196      |\n",
            "|    fps              | 227      |\n",
            "|    time_elapsed     | 567      |\n",
            "|    total_timesteps  | 128763   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.88e-06 |\n",
            "|    n_updates        | 27190    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=130000, episode_reward=4.61 +/- 0.30\n",
            "Episode length: 742.60 +/- 60.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 743      |\n",
            "|    mean_reward      | 4.61     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 130000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.15e-06 |\n",
            "|    n_updates        | 27499    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 719      |\n",
            "|    ep_rew_mean      | 4.62     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 200      |\n",
            "|    fps              | 224      |\n",
            "|    time_elapsed     | 587      |\n",
            "|    total_timesteps  | 131667   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.92e-06 |\n",
            "|    n_updates        | 27916    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 720      |\n",
            "|    ep_rew_mean      | 4.61     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 204      |\n",
            "|    fps              | 224      |\n",
            "|    time_elapsed     | 600      |\n",
            "|    total_timesteps  | 134553   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 9.72e-06 |\n",
            "|    n_updates        | 28638    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 725      |\n",
            "|    ep_rew_mean      | 4.63     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 208      |\n",
            "|    fps              | 224      |\n",
            "|    time_elapsed     | 613      |\n",
            "|    total_timesteps  | 137484   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.06e-05 |\n",
            "|    n_updates        | 29370    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=140000, episode_reward=5.26 +/- 0.00\n",
            "Episode length: 873.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 873      |\n",
            "|    mean_reward      | 5.26     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 140000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.98e-06 |\n",
            "|    n_updates        | 29999    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 731      |\n",
            "|    ep_rew_mean      | 4.68     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 212      |\n",
            "|    fps              | 220      |\n",
            "|    time_elapsed     | 635      |\n",
            "|    total_timesteps  | 140445   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 7.84e-06 |\n",
            "|    n_updates        | 30111    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 724      |\n",
            "|    ep_rew_mean      | 4.63     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 216      |\n",
            "|    fps              | 221      |\n",
            "|    time_elapsed     | 646      |\n",
            "|    total_timesteps  | 143009   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.87e-06 |\n",
            "|    n_updates        | 30752    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 725      |\n",
            "|    ep_rew_mean      | 4.64     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 220      |\n",
            "|    fps              | 221      |\n",
            "|    time_elapsed     | 658      |\n",
            "|    total_timesteps  | 145544   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.86e-06 |\n",
            "|    n_updates        | 31385    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 728      |\n",
            "|    ep_rew_mean      | 4.63     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 224      |\n",
            "|    fps              | 221      |\n",
            "|    time_elapsed     | 672      |\n",
            "|    total_timesteps  | 148924   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.32e-05 |\n",
            "|    n_updates        | 32230    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=150000, episode_reward=4.64 +/- 0.02\n",
            "Episode length: 747.40 +/- 3.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 747      |\n",
            "|    mean_reward      | 4.64     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 150000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.5e-06  |\n",
            "|    n_updates        | 32499    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 726      |\n",
            "|    ep_rew_mean      | 4.63     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 228      |\n",
            "|    fps              | 218      |\n",
            "|    time_elapsed     | 691      |\n",
            "|    total_timesteps  | 151471   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.2e-05  |\n",
            "|    n_updates        | 32867    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 723      |\n",
            "|    ep_rew_mean      | 4.61     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 232      |\n",
            "|    fps              | 219      |\n",
            "|    time_elapsed     | 703      |\n",
            "|    total_timesteps  | 154091   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 8.31e-06 |\n",
            "|    n_updates        | 33522    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 726      |\n",
            "|    ep_rew_mean      | 4.61     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 236      |\n",
            "|    fps              | 219      |\n",
            "|    time_elapsed     | 717      |\n",
            "|    total_timesteps  | 157386   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.11e-05 |\n",
            "|    n_updates        | 34346    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 723      |\n",
            "|    ep_rew_mean      | 4.6      |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 240      |\n",
            "|    fps              | 219      |\n",
            "|    time_elapsed     | 729      |\n",
            "|    total_timesteps  | 159932   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.09e-06 |\n",
            "|    n_updates        | 34982    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=160000, episode_reward=4.16 +/- 0.00\n",
            "Episode length: 653.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 653      |\n",
            "|    mean_reward      | 4.16     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 160000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.58e-05 |\n",
            "|    n_updates        | 34999    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 721      |\n",
            "|    ep_rew_mean      | 4.59     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 244      |\n",
            "|    fps              | 217      |\n",
            "|    time_elapsed     | 748      |\n",
            "|    total_timesteps  | 162668   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.43e-06 |\n",
            "|    n_updates        | 35666    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 716      |\n",
            "|    ep_rew_mean      | 4.56     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 248      |\n",
            "|    fps              | 217      |\n",
            "|    time_elapsed     | 758      |\n",
            "|    total_timesteps  | 165091   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.32e-06 |\n",
            "|    n_updates        | 36272    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 711      |\n",
            "|    ep_rew_mean      | 4.54     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 252      |\n",
            "|    fps              | 217      |\n",
            "|    time_elapsed     | 770      |\n",
            "|    total_timesteps  | 167823   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 7.67e-06 |\n",
            "|    n_updates        | 36955    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=170000, episode_reward=4.93 +/- 0.36\n",
            "Episode length: 849.80 +/- 94.40\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 850      |\n",
            "|    mean_reward      | 4.93     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 170000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.36e-06 |\n",
            "|    n_updates        | 37499    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 700      |\n",
            "|    ep_rew_mean      | 4.48     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 256      |\n",
            "|    fps              | 215      |\n",
            "|    time_elapsed     | 791      |\n",
            "|    total_timesteps  | 170565   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.34e-05 |\n",
            "|    n_updates        | 37641    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 697      |\n",
            "|    ep_rew_mean      | 4.46     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 260      |\n",
            "|    fps              | 215      |\n",
            "|    time_elapsed     | 802      |\n",
            "|    total_timesteps  | 173033   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.14e-05 |\n",
            "|    n_updates        | 38258    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 694      |\n",
            "|    ep_rew_mean      | 4.45     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 264      |\n",
            "|    fps              | 215      |\n",
            "|    time_elapsed     | 814      |\n",
            "|    total_timesteps  | 175801   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 8.29e-06 |\n",
            "|    n_updates        | 38950    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 695      |\n",
            "|    ep_rew_mean      | 4.45     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 268      |\n",
            "|    fps              | 215      |\n",
            "|    time_elapsed     | 827      |\n",
            "|    total_timesteps  | 178649   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 7.58e-06 |\n",
            "|    n_updates        | 39662    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=180000, episode_reward=3.56 +/- 0.05\n",
            "Episode length: 532.20 +/- 10.85\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 532      |\n",
            "|    mean_reward      | 3.56     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 180000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.78e-06 |\n",
            "|    n_updates        | 39999    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 695      |\n",
            "|    ep_rew_mean      | 4.46     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 272      |\n",
            "|    fps              | 214      |\n",
            "|    time_elapsed     | 844      |\n",
            "|    total_timesteps  | 181298   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.4e-06  |\n",
            "|    n_updates        | 40324    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 694      |\n",
            "|    ep_rew_mean      | 4.45     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 276      |\n",
            "|    fps              | 214      |\n",
            "|    time_elapsed     | 856      |\n",
            "|    total_timesteps  | 184046   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000514 |\n",
            "|    n_updates        | 41011    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 702      |\n",
            "|    ep_rew_mean      | 4.49     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 280      |\n",
            "|    fps              | 215      |\n",
            "|    time_elapsed     | 871      |\n",
            "|    total_timesteps  | 187463   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.98e-05 |\n",
            "|    n_updates        | 41865    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 690      |\n",
            "|    ep_rew_mean      | 4.41     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 284      |\n",
            "|    fps              | 215      |\n",
            "|    time_elapsed     | 881      |\n",
            "|    total_timesteps  | 189711   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.87e-05 |\n",
            "|    n_updates        | 42427    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=190000, episode_reward=3.06 +/- 0.06\n",
            "Episode length: 431.40 +/- 12.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 431      |\n",
            "|    mean_reward      | 3.06     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 190000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.42e-05 |\n",
            "|    n_updates        | 42499    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 684      |\n",
            "|    ep_rew_mean      | 4.38     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 288      |\n",
            "|    fps              | 214      |\n",
            "|    time_elapsed     | 896      |\n",
            "|    total_timesteps  | 192152   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.76e-05 |\n",
            "|    n_updates        | 43037    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 686      |\n",
            "|    ep_rew_mean      | 4.39     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 292      |\n",
            "|    fps              | 214      |\n",
            "|    time_elapsed     | 908      |\n",
            "|    total_timesteps  | 194828   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.06e-05 |\n",
            "|    n_updates        | 43706    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 684      |\n",
            "|    ep_rew_mean      | 4.38     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 296      |\n",
            "|    fps              | 214      |\n",
            "|    time_elapsed     | 918      |\n",
            "|    total_timesteps  | 197116   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.32e-05 |\n",
            "|    n_updates        | 44278    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 681      |\n",
            "|    ep_rew_mean      | 4.35     |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    episodes         | 300      |\n",
            "|    fps              | 214      |\n",
            "|    time_elapsed     | 930      |\n",
            "|    total_timesteps  | 199784   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.07e-05 |\n",
            "|    n_updates        | 44945    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=200000, episode_reward=3.09 +/- 0.99\n",
            "Episode length: 438.60 +/- 197.50\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 439      |\n",
            "|    mean_reward      | 3.09     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.1      |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 200000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 8.08e-06 |\n",
            "|    n_updates        | 44999    |\n",
            "----------------------------------\n",
            "Training complete and final model saved at /content/drive/My Drive/tetris_checkpoints/tetris_cnn_exp_v1\n",
            "Mean reward: 2.855 +/- 0.35499295767662775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final model\n",
        "#model.save(\"/content/drive/My Drive/tetris_checkpoints/cnn_tetris_v2_final.zip\")"
      ],
      "metadata": {
        "id": "Umqn6whxXTVe"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}
