{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVpmjguQJV73",
        "outputId": "4bb03f15-0f87-4741-d07f-a685fbccae39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Requirement already satisfied: shimmy<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (0.2.1)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (0.8.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (6.4.4)\n",
            "Requirement already satisfied: gymnasium[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (0.0.4)\n",
            "Requirement already satisfied: autorom~=0.4.2 in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (4.66.5)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2024.7.4)\n",
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ale-py) (1.26.4)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py) (6.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py) (4.12.2)\n",
            "Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.10/dist-packages (2.3.2)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.4.0+cpu)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2024.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: imageio[ffmpeg] in /usr/local/lib/python3.10/dist-packages (2.35.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio[ffmpeg]) (1.26.4)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio[ffmpeg]) (10.4.0)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (from imageio[ffmpeg]) (0.5.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from imageio[ffmpeg]) (5.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg->imageio[ffmpeg]) (71.0.4)\n"
          ]
        }
      ],
      "source": [
        "# Requirements\n",
        "!pip install gymnasium[atari]\n",
        "!pip install gymnasium[accept-rom-license]\n",
        "!pip install ale-py\n",
        "!pip install stable-baselines3\n",
        "!pip install imageio[ffmpeg]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
        "import ale_py\n",
        "from gymnasium.wrappers import TransformReward, TimeLimit\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "_y1IC6kFLhRc"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create custom reward and penalty functions\n",
        "class CustomRewardWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        # Counters for specific actions taken by the agent\n",
        "        self.fire_count = 0\n",
        "        self.left_right_count = 0\n",
        "\n",
        "        # Thresholds for applying penalties and rewards\n",
        "        self.left_right_threshold = 100\n",
        "        self.fire_penalty_threshold = 500\n",
        "\n",
        "        # Penalties and rewards definitions\n",
        "        self.fire_penalty = -0.01  # Penalty for excessive \"FIRE\" actions\n",
        "        self.termination_penalty = -0.1  # Penalty for game over\n",
        "        self.movement_reward = 0.01  # Reward for moving left or right\n",
        "\n",
        "        # Additional bonuses\n",
        "        self.survival_bonus = 0.005  # Bonus for surviving longer\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        # Reset counters when a new episode starts\n",
        "        self.fire_count = 0\n",
        "        self.left_right_count = 0\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "    def step(self, action):\n",
        "        # Execute the action and observe the outcome\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "        # Increment fire count and apply penalty if threshold is exceeded\n",
        "        if action == 1:\n",
        "            self.fire_count += 1\n",
        "            if self.fire_count > self.fire_penalty_threshold:\n",
        "                reward += self.fire_penalty  # Penalize for excessive fire actions\n",
        "\n",
        "        # Increment left/right count and reward movement within threshold\n",
        "        if action == 2 or action == 3:\n",
        "            self.left_right_count += 1\n",
        "            if self.left_right_count <= self.left_right_threshold:\n",
        "                reward += self.movement_reward  # Reward for moving left or right\n",
        "\n",
        "        # Apply bonuses for gameplay strategies\n",
        "        reward += self.survival_bonus  # Bonus for each step the game continues\n",
        "\n",
        "        # Apply termination penalty if the game ends\n",
        "        if terminated:\n",
        "            reward += self.termination_penalty\n",
        "\n",
        "        return obs, reward, terminated, truncated, info"
      ],
      "metadata": {
        "id": "IOjkF29_LkXi"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removed because the incremental height penalty overwhelmed the rest of the rewards\n",
        "# Create custom reward and penalty function\n",
        "#class CustomRewardWrapper(gym.Wrapper):\n",
        "    #def __init__(self, env):\n",
        "        # super().__init__(env)\n",
        "        # Counters for specific actions taken by the agent\n",
        "        #self.fire_count = 0\n",
        "        #self.left_right_count = 0\n",
        "\n",
        "        # Thresholds for applying penalties and rewards\n",
        "        #self.left_right_threshold = 100\n",
        "        #self.fire_penalty_threshold = 500\n",
        "\n",
        "        # Penalties and rewards definitions\n",
        "        #self.fire_penalty = -0.01  # Penalty for excessive \"FIRE\" actions\n",
        "        #self.termination_penalty = -0.1  # Penalty for game over\n",
        "        #self.movement_reward = 0.01  # Reward for moving left or right\n",
        "\n",
        "        # Additional bonuses\n",
        "        #self.low_stack_bonus = 0.02  # Bonus for keeping stack low\n",
        "        #self.survival_bonus = 0.005  # Bonus for surviving longer\n",
        "\n",
        "        # Game specific settings\n",
        "        #self.game_height = 20  # Total height of the Tetris board\n",
        "        #self.height_threshold = self.game_height / 2  # Threshold for applying height penalty\n",
        "        #self.max_height_penalty = -0.02  # Maximum penalty for exceeding height threshold\n",
        "\n",
        "    #def reset(self, **kwargs):\n",
        "        # Reset counters when a new episode starts\n",
        "        #self.fire_count = 0\n",
        "        #self.left_right_count = 0\n",
        "        #return self.env.reset(**kwargs)\n",
        "\n",
        "    #def step(self, action):\n",
        "        # Execute the action and observe the outcome\n",
        "        #obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "        # Increment fire count and apply penalty if threshold is exceeded\n",
        "        #if action == 1:\n",
        "            #self.fire_count += 1\n",
        "            #if self.fire_count > self.fire_penalty_threshold:\n",
        "                #reward += self.fire_penalty  # Penalize for excessive fire actions\n",
        "\n",
        "        # Increment left/right count and reward movement within threshold\n",
        "        #if action == 2 or action == 3:\n",
        "            #self.left_right_count += 1\n",
        "            #if self.left_right_count <= self.left_right_threshold:\n",
        "                #reward += self.movement_reward  # Reward for moving left or right\n",
        "\n",
        "        # Evaluate the board and adjust reward based on the current stack height\n",
        "        #board_reward, low_stack = self.evaluate_board(obs)\n",
        "        #reward += board_reward\n",
        "\n",
        "        # Apply bonuses for gameplay strategies\n",
        "        #if low_stack:\n",
        "            #reward += self.low_stack_bonus  # Bonus for keeping the stack below the threshold\n",
        "        #reward += self.survival_bonus  # Bonus for each step the game continues\n",
        "\n",
        "        # Apply termination penalty if the game ends\n",
        "        #if terminated:\n",
        "            #reward += self.termination_penalty\n",
        "\n",
        "        #return obs, reward, terminated, truncated, info\n",
        "\n",
        "    #def evaluate_board(self, observation):\n",
        "        # Determine filled positions on the board and calculate height penalties\n",
        "        #is_filled = np.any(observation != [0, 0, 0], axis=-1)\n",
        "        #height_penalty = 0\n",
        "        #low_stack = True  # Assume low stack until found otherwise\n",
        "\n",
        "        # Calculate height penalty for each column\n",
        "        #for col in range(is_filled.shape[1]):\n",
        "            #column_data = is_filled[:, col]\n",
        "            #first_filled_index = np.argmax(column_data) if np.any(column_data) else len(column_data)\n",
        "            #column_height = len(column_data) - first_filled_index\n",
        "            #if column_height > self.height_threshold:\n",
        "                #excess_height = column_height - self.height_threshold\n",
        "                #height_penalty += (excess_height / self.height_threshold) * self.max_height_penalty\n",
        "                #low_stack = False  # Set to False if any column exceeds the threshold\n",
        "\n",
        "        #return height_penalty, low_stack"
      ],
      "metadata": {
        "id": "qpEczjUmLuti"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the Atari environment\n",
        "gym.register_envs(ale_py)\n",
        "\n",
        "# Create the Tetris environment and modify with time limit and custom class\n",
        "env = gym.make(\"ALE/Tetris-v5\")\n",
        "#env = TimeLimit(env, max_episode_steps=5000)  # Limit episodes to 5000 steps\n",
        "env = CustomRewardWrapper(env)\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to the model\n",
        "model_path = \"/content/drive/My Drive/tetris_checkpoints/dqn_tetris_v4.zip\"\n",
        "save_path = \"/content/drive/My Drive/tetris_checkpoints/\"\n",
        "\n",
        "# Check if the model exists and load or create a new one\n",
        "if os.path.exists(model_path):\n",
        "    print(\"Loading existing model to continue training.\")\n",
        "    model = DQN.load(model_path, env=env)\n",
        "else:\n",
        "    print(\"No existing model found, training a new model.\")\n",
        "    model = DQN(\n",
        "        policy=\"MlpPolicy\",\n",
        "        env=env,\n",
        "        learning_rate=1e-4,\n",
        "        buffer_size=1000000,\n",
        "        learning_starts=20000,\n",
        "        batch_size=32,\n",
        "        tau=1.0,\n",
        "        gamma=0.99,\n",
        "        train_freq=4,\n",
        "        target_update_interval=10000,\n",
        "        exploration_fraction=0.1,\n",
        "        exploration_final_eps=0.02,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "# Callback to save the best model\n",
        "eval_callback = EvalCallback(\n",
        "    env,\n",
        "    best_model_save_path=save_path,\n",
        "    log_path=save_path,\n",
        "    eval_freq=10000,\n",
        "    deterministic=True,\n",
        "    render=False\n",
        ")\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs('/content/tetris_checkpoints/', exist_ok=True)\n",
        "\n",
        "# Train the model\n",
        "model.learn(total_timesteps=200000, callback=[eval_callback])\n",
        "\n",
        "# Save the final model\n",
        "final_model_path = os.path.join(save_path, \"dqn_tetris_v14\")\n",
        "model.save(final_model_path)\n",
        "print(\"Training complete and final model saved at\", final_model_path)\n",
        "\n",
        "\n",
        "# Evaluate the trained model\n",
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
        "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")\n",
        "\n",
        "# Close the environment\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emHR09jYLzff",
        "outputId": "6594c2c2-d101-4bbc-d828-36073e447887"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loading existing model to continue training.\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Wrapping the env in a VecTransposeImage.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py:414: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x7a5487d40a00> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7a5487d405e0>\n",
            "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 590      |\n",
            "|    ep_rew_mean      | 4.1      |\n",
            "|    exploration_rate | 0.884    |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 882      |\n",
            "|    time_elapsed     | 2        |\n",
            "|    total_timesteps  | 2362     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 563      |\n",
            "|    ep_rew_mean      | 3.84     |\n",
            "|    exploration_rate | 0.779    |\n",
            "| time/               |          |\n",
            "|    episodes         | 8        |\n",
            "|    fps              | 879      |\n",
            "|    time_elapsed     | 5        |\n",
            "|    total_timesteps  | 4501     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 544      |\n",
            "|    ep_rew_mean      | 3.7      |\n",
            "|    exploration_rate | 0.68     |\n",
            "| time/               |          |\n",
            "|    episodes         | 12       |\n",
            "|    fps              | 880      |\n",
            "|    time_elapsed     | 7        |\n",
            "|    total_timesteps  | 6524     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 556      |\n",
            "|    ep_rew_mean      | 3.74     |\n",
            "|    exploration_rate | 0.564    |\n",
            "| time/               |          |\n",
            "|    episodes         | 16       |\n",
            "|    fps              | 884      |\n",
            "|    time_elapsed     | 10       |\n",
            "|    total_timesteps  | 8902     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=3.62 +/- 0.00\n",
            "Episode length: 545.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 545      |\n",
            "|    mean_reward      | 3.62     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.51     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 10000    |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 562      |\n",
            "|    ep_rew_mean      | 3.77     |\n",
            "|    exploration_rate | 0.449    |\n",
            "| time/               |          |\n",
            "|    episodes         | 20       |\n",
            "|    fps              | 608      |\n",
            "|    time_elapsed     | 18       |\n",
            "|    total_timesteps  | 11238    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 554      |\n",
            "|    ep_rew_mean      | 3.72     |\n",
            "|    exploration_rate | 0.348    |\n",
            "| time/               |          |\n",
            "|    episodes         | 24       |\n",
            "|    fps              | 639      |\n",
            "|    time_elapsed     | 20       |\n",
            "|    total_timesteps  | 13301    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 541      |\n",
            "|    ep_rew_mean      | 3.65     |\n",
            "|    exploration_rate | 0.258    |\n",
            "| time/               |          |\n",
            "|    episodes         | 28       |\n",
            "|    fps              | 662      |\n",
            "|    time_elapsed     | 22       |\n",
            "|    total_timesteps  | 15135    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 535      |\n",
            "|    ep_rew_mean      | 3.62     |\n",
            "|    exploration_rate | 0.161    |\n",
            "| time/               |          |\n",
            "|    episodes         | 32       |\n",
            "|    fps              | 683      |\n",
            "|    time_elapsed     | 25       |\n",
            "|    total_timesteps  | 17129    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 548      |\n",
            "|    ep_rew_mean      | 3.67     |\n",
            "|    exploration_rate | 0.0338   |\n",
            "| time/               |          |\n",
            "|    episodes         | 36       |\n",
            "|    fps              | 705      |\n",
            "|    time_elapsed     | 27       |\n",
            "|    total_timesteps  | 19719    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=3.62 +/- 0.00\n",
            "Episode length: 545.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 545      |\n",
            "|    mean_reward      | 3.62     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 20000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 570      |\n",
            "|    ep_rew_mean      | 3.81     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 40       |\n",
            "|    fps              | 437      |\n",
            "|    time_elapsed     | 52       |\n",
            "|    total_timesteps  | 22820    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.69e-05 |\n",
            "|    n_updates        | 180704   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 605      |\n",
            "|    ep_rew_mean      | 3.98     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 44       |\n",
            "|    fps              | 342      |\n",
            "|    time_elapsed     | 77       |\n",
            "|    total_timesteps  | 26612    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 7.82e-05 |\n",
            "|    n_updates        | 181652   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 621      |\n",
            "|    ep_rew_mean      | 4.05     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 48       |\n",
            "|    fps              | 300      |\n",
            "|    time_elapsed     | 99       |\n",
            "|    total_timesteps  | 29812    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.28e-05 |\n",
            "|    n_updates        | 182452   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=3.26 +/- 0.00\n",
            "Episode length: 473.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 473      |\n",
            "|    mean_reward      | 3.26     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 30000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.54e-05 |\n",
            "|    n_updates        | 182499   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 639      |\n",
            "|    ep_rew_mean      | 4.16     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 52       |\n",
            "|    fps              | 261      |\n",
            "|    time_elapsed     | 126      |\n",
            "|    total_timesteps  | 33236    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 7.57e-05 |\n",
            "|    n_updates        | 183308   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 661      |\n",
            "|    ep_rew_mean      | 4.26     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 56       |\n",
            "|    fps              | 242      |\n",
            "|    time_elapsed     | 152      |\n",
            "|    total_timesteps  | 36992    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 9.89e-06 |\n",
            "|    n_updates        | 184247   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=3.32 +/- 0.00\n",
            "Episode length: 485.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 485      |\n",
            "|    mean_reward      | 3.32     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 40000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.67e-05 |\n",
            "|    n_updates        | 184999   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 676      |\n",
            "|    ep_rew_mean      | 4.35     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 60       |\n",
            "|    fps              | 223      |\n",
            "|    time_elapsed     | 181      |\n",
            "|    total_timesteps  | 40573    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.99e-05 |\n",
            "|    n_updates        | 185143   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 681      |\n",
            "|    ep_rew_mean      | 4.37     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 64       |\n",
            "|    fps              | 215      |\n",
            "|    time_elapsed     | 202      |\n",
            "|    total_timesteps  | 43577    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000133 |\n",
            "|    n_updates        | 185894   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 697      |\n",
            "|    ep_rew_mean      | 4.45     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 68       |\n",
            "|    fps              | 207      |\n",
            "|    time_elapsed     | 228      |\n",
            "|    total_timesteps  | 47419    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.56e-05 |\n",
            "|    n_updates        | 186854   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=3.14 +/- 0.00\n",
            "Episode length: 449.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 449      |\n",
            "|    mean_reward      | 3.14     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 50000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.17e-05 |\n",
            "|    n_updates        | 187499   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 704      |\n",
            "|    ep_rew_mean      | 4.51     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 72       |\n",
            "|    fps              | 198      |\n",
            "|    time_elapsed     | 255      |\n",
            "|    total_timesteps  | 50713    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.47e-05 |\n",
            "|    n_updates        | 187678   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 713      |\n",
            "|    ep_rew_mean      | 4.55     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 76       |\n",
            "|    fps              | 194      |\n",
            "|    time_elapsed     | 279      |\n",
            "|    total_timesteps  | 54157    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.42e-05 |\n",
            "|    n_updates        | 188539   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 716      |\n",
            "|    ep_rew_mean      | 4.56     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 80       |\n",
            "|    fps              | 190      |\n",
            "|    time_elapsed     | 300      |\n",
            "|    total_timesteps  | 57288    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00631  |\n",
            "|    n_updates        | 189321   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=3.02 +/- 0.00\n",
            "Episode length: 425.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 425      |\n",
            "|    mean_reward      | 3.02     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 60000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.93e-05 |\n",
            "|    n_updates        | 189999   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 729      |\n",
            "|    ep_rew_mean      | 4.63     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 84       |\n",
            "|    fps              | 185      |\n",
            "|    time_elapsed     | 330      |\n",
            "|    total_timesteps  | 61206    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.44e-05 |\n",
            "|    n_updates        | 190301   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 734      |\n",
            "|    ep_rew_mean      | 4.65     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 88       |\n",
            "|    fps              | 182      |\n",
            "|    time_elapsed     | 353      |\n",
            "|    total_timesteps  | 64578    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000213 |\n",
            "|    n_updates        | 191144   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 742      |\n",
            "|    ep_rew_mean      | 4.69     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 92       |\n",
            "|    fps              | 180      |\n",
            "|    time_elapsed     | 379      |\n",
            "|    total_timesteps  | 68306    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.92e-05 |\n",
            "|    n_updates        | 192076   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=3.62 +/- 0.00\n",
            "Episode length: 545.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 545      |\n",
            "|    mean_reward      | 3.62     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 70000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.08e-05 |\n",
            "|    n_updates        | 192499   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 744      |\n",
            "|    ep_rew_mean      | 4.71     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 96       |\n",
            "|    fps              | 175      |\n",
            "|    time_elapsed     | 406      |\n",
            "|    total_timesteps  | 71446    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.79e-05 |\n",
            "|    n_updates        | 192861   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 751      |\n",
            "|    ep_rew_mean      | 4.74     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 100      |\n",
            "|    fps              | 174      |\n",
            "|    time_elapsed     | 430      |\n",
            "|    total_timesteps  | 75130    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.82e-05 |\n",
            "|    n_updates        | 193782   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 760      |\n",
            "|    ep_rew_mean      | 4.77     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 104      |\n",
            "|    fps              | 172      |\n",
            "|    time_elapsed     | 452      |\n",
            "|    total_timesteps  | 78318    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.33e-05 |\n",
            "|    n_updates        | 194579   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=3.26 +/- 0.00\n",
            "Episode length: 473.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 473      |\n",
            "|    mean_reward      | 3.26     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 80000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.8e-05  |\n",
            "|    n_updates        | 194999   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 775      |\n",
            "|    ep_rew_mean      | 4.86     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 108      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 483      |\n",
            "|    total_timesteps  | 82027    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.91e-05 |\n",
            "|    n_updates        | 195506   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 793      |\n",
            "|    ep_rew_mean      | 4.94     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 112      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 508      |\n",
            "|    total_timesteps  | 85787    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.22e-05 |\n",
            "|    n_updates        | 196446   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 800      |\n",
            "|    ep_rew_mean      | 4.98     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 116      |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 530      |\n",
            "|    total_timesteps  | 88875    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.06e-05 |\n",
            "|    n_updates        | 197218   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=3.51 +/- 0.00\n",
            "Episode length: 545.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 545      |\n",
            "|    mean_reward      | 3.51     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 90000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00293  |\n",
            "|    n_updates        | 197499   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 816      |\n",
            "|    ep_rew_mean      | 5.08     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 120      |\n",
            "|    fps              | 164      |\n",
            "|    time_elapsed     | 563      |\n",
            "|    total_timesteps  | 92875    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.88e-05 |\n",
            "|    n_updates        | 198218   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 825      |\n",
            "|    ep_rew_mean      | 5.12     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 124      |\n",
            "|    fps              | 164      |\n",
            "|    time_elapsed     | 582      |\n",
            "|    total_timesteps  | 95775    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 7.26e-06 |\n",
            "|    n_updates        | 198943   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 836      |\n",
            "|    ep_rew_mean      | 5.18     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 128      |\n",
            "|    fps              | 163      |\n",
            "|    time_elapsed     | 602      |\n",
            "|    total_timesteps  | 98745    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.47e-05 |\n",
            "|    n_updates        | 199686   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=3.26 +/- 0.00\n",
            "Episode length: 473.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 473      |\n",
            "|    mean_reward      | 3.26     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 100000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000103 |\n",
            "|    n_updates        | 199999   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 853      |\n",
            "|    ep_rew_mean      | 5.27     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 132      |\n",
            "|    fps              | 162      |\n",
            "|    time_elapsed     | 631      |\n",
            "|    total_timesteps  | 102413   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.62e-05 |\n",
            "|    n_updates        | 200603   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 860      |\n",
            "|    ep_rew_mean      | 5.31     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 136      |\n",
            "|    fps              | 161      |\n",
            "|    time_elapsed     | 654      |\n",
            "|    total_timesteps  | 105749   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 7.76e-05 |\n",
            "|    n_updates        | 201437   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 859      |\n",
            "|    ep_rew_mean      | 5.29     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 140      |\n",
            "|    fps              | 161      |\n",
            "|    time_elapsed     | 674      |\n",
            "|    total_timesteps  | 108725   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.71e-05 |\n",
            "|    n_updates        | 202181   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=110000, episode_reward=3.32 +/- 0.00\n",
            "Episode length: 485.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 485      |\n",
            "|    mean_reward      | 3.32     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 110000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 8.14e-05 |\n",
            "|    n_updates        | 202499   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 857      |\n",
            "|    ep_rew_mean      | 5.29     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 144      |\n",
            "|    fps              | 159      |\n",
            "|    time_elapsed     | 702      |\n",
            "|    total_timesteps  | 112339   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.74e-05 |\n",
            "|    n_updates        | 203084   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 850      |\n",
            "|    ep_rew_mean      | 5.25     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 148      |\n",
            "|    fps              | 159      |\n",
            "|    time_elapsed     | 719      |\n",
            "|    total_timesteps  | 114814   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.13e-05 |\n",
            "|    n_updates        | 203703   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 848      |\n",
            "|    ep_rew_mean      | 5.23     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 152      |\n",
            "|    fps              | 159      |\n",
            "|    time_elapsed     | 740      |\n",
            "|    total_timesteps  | 118014   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.25e-05 |\n",
            "|    n_updates        | 204503   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=120000, episode_reward=3.18 +/- 0.00\n",
            "Episode length: 457.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 457      |\n",
            "|    mean_reward      | 3.18     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 120000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 9.32e-06 |\n",
            "|    n_updates        | 204999   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 845      |\n",
            "|    ep_rew_mean      | 5.23     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 156      |\n",
            "|    fps              | 158      |\n",
            "|    time_elapsed     | 769      |\n",
            "|    total_timesteps  | 121538   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.15e-05 |\n",
            "|    n_updates        | 205384   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 842      |\n",
            "|    ep_rew_mean      | 5.2      |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 160      |\n",
            "|    fps              | 157      |\n",
            "|    time_elapsed     | 791      |\n",
            "|    total_timesteps  | 124750   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000112 |\n",
            "|    n_updates        | 206187   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 848      |\n",
            "|    ep_rew_mean      | 5.23     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 164      |\n",
            "|    fps              | 157      |\n",
            "|    time_elapsed     | 816      |\n",
            "|    total_timesteps  | 128390   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 9.36e-05 |\n",
            "|    n_updates        | 207097   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=130000, episode_reward=2.06 +/- 0.00\n",
            "Episode length: 232.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 232      |\n",
            "|    mean_reward      | 2.06     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 130000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 7.33e-05 |\n",
            "|    n_updates        | 207499   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 839      |\n",
            "|    ep_rew_mean      | 5.2      |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 168      |\n",
            "|    fps              | 156      |\n",
            "|    time_elapsed     | 838      |\n",
            "|    total_timesteps  | 131326   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.87e-05 |\n",
            "|    n_updates        | 207831   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 830      |\n",
            "|    ep_rew_mean      | 5.13     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 172      |\n",
            "|    fps              | 156      |\n",
            "|    time_elapsed     | 854      |\n",
            "|    total_timesteps  | 133668   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.53e-05 |\n",
            "|    n_updates        | 208416   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 820      |\n",
            "|    ep_rew_mean      | 5.08     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 176      |\n",
            "|    fps              | 156      |\n",
            "|    time_elapsed     | 870      |\n",
            "|    total_timesteps  | 136149   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.24e-05 |\n",
            "|    n_updates        | 209037   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 811      |\n",
            "|    ep_rew_mean      | 5.04     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 180      |\n",
            "|    fps              | 156      |\n",
            "|    time_elapsed     | 885      |\n",
            "|    total_timesteps  | 138390   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.63e-05 |\n",
            "|    n_updates        | 209597   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=140000, episode_reward=0.32 +/- 0.00\n",
            "Episode length: 84.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 84       |\n",
            "|    mean_reward      | 0.32     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 140000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.55e-05 |\n",
            "|    n_updates        | 209999   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 808      |\n",
            "|    ep_rew_mean      | 5.02     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 184      |\n",
            "|    fps              | 155      |\n",
            "|    time_elapsed     | 910      |\n",
            "|    total_timesteps  | 141982   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 8.17e-05 |\n",
            "|    n_updates        | 210495   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 808      |\n",
            "|    ep_rew_mean      | 5.03     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 188      |\n",
            "|    fps              | 155      |\n",
            "|    time_elapsed     | 933      |\n",
            "|    total_timesteps  | 145394   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.45e-05 |\n",
            "|    n_updates        | 211348   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 802      |\n",
            "|    ep_rew_mean      | 5        |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 192      |\n",
            "|    fps              | 155      |\n",
            "|    time_elapsed     | 953      |\n",
            "|    total_timesteps  | 148550   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.8e-05  |\n",
            "|    n_updates        | 212137   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=150000, episode_reward=3.48 +/- 0.00\n",
            "Episode length: 517.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 517      |\n",
            "|    mean_reward      | 3.48     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 150000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.45e-05 |\n",
            "|    n_updates        | 212499   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 799      |\n",
            "|    ep_rew_mean      | 4.98     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 196      |\n",
            "|    fps              | 154      |\n",
            "|    time_elapsed     | 977      |\n",
            "|    total_timesteps  | 151342   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.45e-05 |\n",
            "|    n_updates        | 212835   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 785      |\n",
            "|    ep_rew_mean      | 4.91     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 200      |\n",
            "|    fps              | 154      |\n",
            "|    time_elapsed     | 992      |\n",
            "|    total_timesteps  | 153634   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.02e-05 |\n",
            "|    n_updates        | 213408   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 794      |\n",
            "|    ep_rew_mean      | 4.96     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 204      |\n",
            "|    fps              | 154      |\n",
            "|    time_elapsed     | 1021     |\n",
            "|    total_timesteps  | 157682   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.57e-05 |\n",
            "|    n_updates        | 214420   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=160000, episode_reward=3.32 +/- 0.00\n",
            "Episode length: 485.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 485      |\n",
            "|    mean_reward      | 3.32     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 160000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000134 |\n",
            "|    n_updates        | 214999   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 788      |\n",
            "|    ep_rew_mean      | 4.93     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 208      |\n",
            "|    fps              | 153      |\n",
            "|    time_elapsed     | 1047     |\n",
            "|    total_timesteps  | 160789   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.8e-05  |\n",
            "|    n_updates        | 215197   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 776      |\n",
            "|    ep_rew_mean      | 4.87     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 212      |\n",
            "|    fps              | 153      |\n",
            "|    time_elapsed     | 1064     |\n",
            "|    total_timesteps  | 163349   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.54e-05 |\n",
            "|    n_updates        | 215837   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 768      |\n",
            "|    ep_rew_mean      | 4.83     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 216      |\n",
            "|    fps              | 153      |\n",
            "|    time_elapsed     | 1079     |\n",
            "|    total_timesteps  | 165638   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000179 |\n",
            "|    n_updates        | 216409   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 760      |\n",
            "|    ep_rew_mean      | 4.77     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 220      |\n",
            "|    fps              | 153      |\n",
            "|    time_elapsed     | 1100     |\n",
            "|    total_timesteps  | 168842   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000256 |\n",
            "|    n_updates        | 217210   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=170000, episode_reward=3.26 +/- 0.00\n",
            "Episode length: 473.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 473      |\n",
            "|    mean_reward      | 3.26     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 170000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.52e-05 |\n",
            "|    n_updates        | 217499   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 767      |\n",
            "|    ep_rew_mean      | 4.82     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 224      |\n",
            "|    fps              | 152      |\n",
            "|    time_elapsed     | 1129     |\n",
            "|    total_timesteps  | 172511   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.33e-05 |\n",
            "|    n_updates        | 218127   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 766      |\n",
            "|    ep_rew_mean      | 4.81     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 228      |\n",
            "|    fps              | 152      |\n",
            "|    time_elapsed     | 1147     |\n",
            "|    total_timesteps  | 175343   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00014  |\n",
            "|    n_updates        | 218835   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 752      |\n",
            "|    ep_rew_mean      | 4.73     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 232      |\n",
            "|    fps              | 152      |\n",
            "|    time_elapsed     | 1162     |\n",
            "|    total_timesteps  | 177581   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.35e-05 |\n",
            "|    n_updates        | 219395   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=180000, episode_reward=3.62 +/- 0.00\n",
            "Episode length: 545.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 545      |\n",
            "|    mean_reward      | 3.62     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 180000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000143 |\n",
            "|    n_updates        | 219999   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 761      |\n",
            "|    ep_rew_mean      | 4.79     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 236      |\n",
            "|    fps              | 151      |\n",
            "|    time_elapsed     | 1196     |\n",
            "|    total_timesteps  | 181822   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 9.35e-06 |\n",
            "|    n_updates        | 220455   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 762      |\n",
            "|    ep_rew_mean      | 4.79     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 240      |\n",
            "|    fps              | 151      |\n",
            "|    time_elapsed     | 1218     |\n",
            "|    total_timesteps  | 184934   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.48e-05 |\n",
            "|    n_updates        | 221233   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 761      |\n",
            "|    ep_rew_mean      | 4.78     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 244      |\n",
            "|    fps              | 151      |\n",
            "|    time_elapsed     | 1242     |\n",
            "|    total_timesteps  | 188482   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.56e-05 |\n",
            "|    n_updates        | 222120   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=190000, episode_reward=3.32 +/- 0.00\n",
            "Episode length: 485.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 485      |\n",
            "|    mean_reward      | 3.32     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 190000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.92e-05 |\n",
            "|    n_updates        | 222499   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 778      |\n",
            "|    ep_rew_mean      | 4.87     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 248      |\n",
            "|    fps              | 150      |\n",
            "|    time_elapsed     | 1276     |\n",
            "|    total_timesteps  | 192623   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.91e-05 |\n",
            "|    n_updates        | 223155   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 771      |\n",
            "|    ep_rew_mean      | 4.84     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 252      |\n",
            "|    fps              | 150      |\n",
            "|    time_elapsed     | 1293     |\n",
            "|    total_timesteps  | 195157   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.6e-05  |\n",
            "|    n_updates        | 223789   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 769      |\n",
            "|    ep_rew_mean      | 4.81     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 256      |\n",
            "|    fps              | 150      |\n",
            "|    time_elapsed     | 1315     |\n",
            "|    total_timesteps  | 198425   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.82e-05 |\n",
            "|    n_updates        | 224606   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=200000, episode_reward=3.26 +/- 0.00\n",
            "Episode length: 473.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 473      |\n",
            "|    mean_reward      | 3.26     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 200000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.55e-05 |\n",
            "|    n_updates        | 224999   |\n",
            "----------------------------------\n",
            "Training complete and final model saved at /content/drive/My Drive/tetris_checkpoints/dqn_tetris_v14\n",
            "Mean reward: 3.2649999999999997 +/- 4.440892098500626e-16\n"
          ]
        }
      ]
    }
  ]
}